%% Version 4.3.2, 25 August 2014
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Template.tex --  LaTeX-based template for submissions to the 
% American Meteorological Society
%
% Template developed by Amy Hendrickson, 2013, TeXnology Inc., 
% amyh@texnology.com, http://www.texnology.com
% following earlier work by Brian Papa, American Meteorological Society
%
% Email questions to latex@ametsoc.org.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% PREAMBLE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% Start with one of the following:
% DOUBLE-SPACED VERSION FOR SUBMISSION TO THE AMS
\documentclass[twocol]{ametsoc}

% TWO-COLUMN JOURNAL PAGE LAYOUT---FOR AUTHOR USE ONLY
% \documentclass[twocol]{ametsoc}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% To be entered only if twocol option is used

\journal{jamc}

%  Please choose a journal abbreviation to use above from the following list:
% 
%   jamc     (Journal of Applied Meteorology and Climatology)
%   jtech     (Journal of Atmospheric and Oceanic Technology)
%   jhm      (Journal of Hydrometeorology)
%   jpo     (Journal of Physical Oceanography)
%   jas      (Journal of Atmospheric Sciences)	
%   jcli      (Journal of Climate)
%   mwr      (Monthly Weather Review)
%   wcas      (Weather, Climate, and Society)
%   waf       (Weather and Forecasting)
%   bams (Bulletin of the American Meteorological Society)
%   ei    (Earth Interactions)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Citations should be of the form ``author year''  not ``author, year''
\bibpunct{(}{)}{;}{a}{}{,}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% To be entered by author:

%% May use \\ to break lines in title:

\title{Verifying Operational Forecasts of Land-Sea Breeze and Boundary Layer Mixing Processes}

%%% Enter authors' names, as you see in this example:
%%% Use \correspondingauthor{} and \thanks{Current Affiliation:...}
%%% immediately following the appropriate author.
%%%
%%% Note that the \correspondingauthor{} command is NECESSARY.
%%% The \thanks{} commands are OPTIONAL.

    %\authors{Author One\correspondingauthor{Author One, 
    % American Meteorological Society, 
    % 45 Beacon St., Boston, MA 02108.}
% and Author Two\thanks{Current affiliation: American Meteorological Society, 
    % 45 Beacon St., Boston, MA 02108.}}

\authors{Ewan Short\correspondingauthor{School of Earth Sciences, The University of Melbourne, Melbourne, Victoria, Australia.}} 

\email{shorte1@student.unimelb.edu.au}

%% Follow this form:
    % \affiliation{American Meteorological Society, 
    % Boston, Massachusetts.}

\affiliation{School of Earth Sciences, and ARC Centre of Excellence for Climate Extremes, The University of Melbourne, Melbourne, Victoria, Australia.}

%% Follow this form:
    %\email{latex@ametsoc.org}

%\email{}

%% If appropriate, add additional authors, different affiliations:
    %\extraauthor{Extra Author}
    %\extraaffil{Affiliation, City, State/Province, Country}

\extraauthor{Benjamin \textcolor{red}{?}. Price}

\extraaffil{Bureau of Meteorology, Casuarina, Northern Territory, Australia}

\extraauthor{Alexei Hider}

\extraaffil{Bureau of Meteorology, Melbourne, Victoria, Australia}

\DeclareMathOperator{\mse}{mse} 
\DeclareMathOperator{\covar}{covar} 
\DeclareMathOperator{\var}{var} 
\DeclareMathOperator{\pr}{Pr} 

%\extraauthor{}
%\extraaffil{}

%% May repeat for a additional authors/affiliations:

%\extraauthor{}
%\extraaffil{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ABSTRACT
%
% Enter your abstract here
% Abstracts should not exceed 250 words in length!
%
% For BAMS authors only: If your article requires a Capsule Summary, please place the capsule text at the end of your abstract
% and identify it as the capsule. Example: This is the end of the abstract. (Capsule Summary) This is the capsule summary. 

% Run "latexdiff --append-context2cmd="abstract" short18_diurnal_cycles_winds.tex short18_diurnal_cycles_winds_revised.tex > short18_diurnal_cycles_winds_tracked_changes.tex" to track changes

\abstract{Forecasts issued by the Australian Bureau of Meteorology are based on model data that is edited by human forecasters. Two types of edits are commonly made to the wind fields. These edits aim to improve how boundary layer mixing processes and the land-sea breeze are resolved in the forecast. In this study we compare the diurnally varying component of the edited wind forecast, with those of station observations and unedited model datasets, to assess changes to bias and mean absolute error resulting from the edits. We consider coastal locations across Australia over June, July and August 2018, assessing performance at three different spatial scales. The results show that the edited forecast generally only produces a lower mean absolute error than model guidance at the coarsest spatial scale (1000 - 2000 km), but can achieve lower seasonal biases over all three spatial scales. However, the edited forecast only reduces errors at particular times and locations, and rarely produces lower errors than all model guidance products simultaneously. This suggests that forecaster skill lies mostly in making the choice of model guidance, rather than in making edits. To better diagnose the causes of errors in the diurnal wind cycles, we fit a modified ellipse to the climatological diurnal cycle hodographs. Performance varies with location for multiple reasons, including biases in the directions sea-breezes approach coastlines, amplitude and shape biases in the hodographs, and disagreement as to whether sea-breeze or boundary layer mixing processes contribute most to the diurnal cycle.}

\usepackage{comment}

\begin{document}

\maketitle

\section{Introduction}
\label{Sec:Introduction}
Modern weather forecasts are typically produced by models in conjunction with human forecasters. Forecasters working for the Australian Bureau of Meteorology (BoM) construct a seven day forecast by loading model data into a software package called the Graphical Forecast Editor (GFE), then editing this model data using tools within the GFE. Forecasters working for the United States National Weather Service also use GFE, utilising a very similar approach. Forecasters can choose which model to base their forecast on, and refer to this as a choice of \textit{model guidance}. Edits are typically made to account for processes that are under-resolved at the resolutions of the model guidance products, or to correct for perceived biases of the model guidance being used. The resulting gridded forecast datasets are provided to the public through the BoM's online MetEye data browser \citep{bomMetEye19}; the gridded forecast datasets are also translated into text and icon forecasts algorithmically.  

Australian forecasters generally make two types of edits to the surface wind fields on a routine daily basis. The first is to edit the surface winds after sunrise at locations where the forecaster believes the model guidance is providing a poor representation of boundary layer mixing processes. Boundary layer mixing occurs as the land surface heats up, producing an unstable boundary layer which transports momentum downward to the surface layer. Before this mixing occurs, winds are typically both weaker and ageostrophically oriented due to surface friction \citep{lee18}, and so mixing can affect both the speed and direction of the surface winds. Australian forecasters perform these edits using a GFE tool which allows them to specify a region over which to apply the edit, a height $z$ and a percentage $p$, with the tool then calculating a weighted average of the surface winds and winds at $z$ weighted by $p$.

The second type of edit involves changing the afternoon and evening surface winds around those coastlines where the forecaster believes the model guidance is resolving the sea-breeze poorly. Similarly to with boundary layer mixing, these edits are performed using a GFE tool that allows forecasters to trace out the relevant coastline graphically, choose a wind speed and a time, with the tool then smoothly blending in winds perpendicular to the traced coastline at the given speed and time.

\begin{table}
\begin{center}
\begin{tabular}{l |c|c}
Airport & Austral Summer & Austral Winter \\
\hline
Darwin  & 6.3 kn& 6.2 kn \\
Brisbane  & 8.6 kn& 7.0 kn \\
Perth  & 11.3 kn& 7.9 kn \\
Sydney  & 12.2 kn & 10.2 kn \\
Adelaide  & 9.5 kn & 10.3 kn \\
Canberra  & 7.4 kn & 7.9 kn \\
Melbourne  & 10.0 kn & 12.1 kn \\
Hobart & 10.0 kn & 8.7 kn
\end{tabular}
\caption{Average 10 m wind speeds for austral winter (June, July August) 2018, and austral summer (December, January, February) 2017/18 across the eight Australian capital city airport weather stations.}
\label{Tab:Speeds}
\end{center}
\end{table}

Forecasters, and the weather services that employ them, have good reasons for ensuring the diurnally varying component of their wind forecasts are as accurate as possible. In addition to the significant contribution diurnal wind cycles make to overall wind fields \citep[e.g.][]{dai99}, diurnal wind cycles are important for the ventilation of pollution, with sea-breezes transporting clean maritime air inland, where it helps flush polluted air out of the boundary layer \citep{miller03, physick92}. Furthermore, diurnal wind cycles affect the function of wind turbines \citep{englberger18} and the design of wind farms \citep{abkar16}, as daily patterns of boundary layer stability affect turbine wake turbulence, and the losses in wind power that result.

To our knowledge, no published work has assessed the diurnal component of human edited forecasts, although some previous studies have assessed the performance of different operational models at specific locations. \citet{svensson11} examined thirty different operational model simulations, including models from most major forecasting centres utilising most commonly used boundary layer parametrisation schemes, and compared their performance with a large eddy simulation (LES), and observations at Kansas, USA, during October 1999. They found that both the models and LES failed to capture the sudden $\approx 6$ kn jump in wind speeds shortly after sunrise, and underestimated morning low level turbulence and wind speeds. Other studies have assessed near-surface wind forecasts, verifying the total wind speeds, not just the diurnal component. \citet{pinson12} studied the 10 m wind speeds resolved by the European Centre for Medium Range Weather Forecasting (ECMWF) operational model ensemble across western Europe over December, January, February 2008/09. They found that the worst performing regions were coastal and mountainous areas, and attributed this to the small scale processes, e.g. sea and mountain breezes, that are under-resolved at ECMWF's coarse 50km spatial resolution.

Any attempt to validate model data against observations must confront the \textit{representation problem} \citep[e.g.][]{zaron06}. Because models cannot resolve physical processes occurring at sub-grid scales, a value predicted by a model for a given grid-cell must be interpreted as a prediction of the filtered, or Reynolds averaged value over that grid-cell. Therefore, comparing model data with observational data can be an unfair test of model performance, and for this reason model forecasts are often verified against reanalysis hind-casts that use the same model \citep[e.g.][]{lynch14}.

However, the way the representation problem applies to the verification of forecasts issued to the public is more nuanced. In this case, a forecast issued by a national weather service is attempting to represent either reality itself, or the filtered version of reality \textit{that is of interest to the end users.} Thus, \citet{pinson12} disregarded the representation problem entirely, arguing that the end user is not interested in spatiotemporal scales of models, only the ``best forecast" at the time and place of their choice. However, different users will have different ideas about what the ``best" forecast entails. Some users may desire a forecast that minimises absolute error between the forecast and observations, others a forecast that most accurately reproduces the observed wind speed distribution, regardless of temporal details. Ideally, operational forecasts should therefore be assessed according to the specific representation needs of particular end users. 

This presents a challenge for the verification of nationally issued forecasts, as these forecasts are utilised by a variety of end users with diverse representation needs. Moreover, the BoM's forecast is formed from model datasets with different resolutions, and the choice of model guidance can change even over the course of a single day, (e.g.~Fig.~\ref{Fig:case_studies_nt} b), making it difficult to determine what the Official forecast intends to represent. Note the BoM's Official gridded wind forecast exists on a 3 or 6 km, one hour spatiotemporal grid, but is provided to the public through MetEye \citep{bomMetEye19} on a 6 km spatial grid, with wind values given only every 3 hours; hourly data is also available, but is less readily accessible to the public. Furthermore, the BoM verifies its wind forecasts by comparing Official forecast values at each hour UTC, with 10 minute averages of station observations at each hour, with station data typically averaged over the 10 minutes leading up to each hour. These practices suggest that either the Official forecast intends to represent wind fields Reynolds averaged over 6 km, 10 minute spatiotemporal grid cells, or that these averages reflect the representation needs of the typical user. 

Related to the representation problem is the question of how mesoscale models, which run at spatial resolutions of between 1 and 10 km, should be verified. The BoM now regularly runs mesoscale models over some Australian capital cities as part of its daily forecasting routine, and the edits performed by human forecasters are also often mesoscale in nature. Note that mesoscale models resolve topography and its effects on the atmosphere in more detail, and explicitly simulate most convective and boundary layer processes. In this sense they are more realistic than coarser scale models, although they can actually perform worse than coarse models on standard verification scores whenever there are timing or location differences between features in the models and in observations. \citet{mass02} found that in the northwestern United States, mean square errors in forecasts produced using mesoscale models decreased with resolution down to 10 to 15 km, whereas in the eastern United States where the topography is much flatter, this threshold was considerably larger, at 20 to 40 km. 

\citet{mass02} therefore argued that existing verification approaches needed reform, suggesting that verification could instead be performed on spatially or temporally averaged parameters, an approach now known as \textit{upscaling} \citep{ebert08}. Alternatively, \citet{mass02} argued that ``feature based" identification metrics be developed, which reward models for realistically simulating atmospheric features, even if the timing or location of these features is incorrect. \citet{rife05} developed such a method for the verification of surface winds, defining a wind ``object" as a wind change of at least one standard deviation occurring within a 12 hour interval, then assessing whether a mesoscale model could replicate the ``objects" present in observations.   

The present study has two goals. First, to describe a method for comparing the diurnal cycles of human edited wind forecasts to those of unedited model guidance forecasts, in order to assess where and when human edits produce an increase in accuracy, and to do so in a way that respects the representation and mesoscale verification challenges discussed above. Second, to apply this methodology across Australian coastal locations to better understand the performance of both boundary layer mixing and land-sea breeze forecaster edits. The remainder of this paper is organised as follows. Section \ref{Sec:Methods} describes the methodology and datasets to which it is applied, section \ref{Sec:Results} provides results, and sections \ref{Sec:Discussion} and \ref{Sec:Conclusion} provide a discussion and a conclusion, respectively.

\section{Data and Methods} \label{Sec:Methods}
This study compares both human edited and unedited Australian Bureau of Meteorology (BoM) wind forecasts with automatic weather station (AWS) data across Australia. The comparison is performed by first isolating the diurnal perturbations of each dataset, then comparing these perturbations on an hour-by-hour basis.

\subsection{Data}
Four datasets are considered in this study; the human edited Official BoM wind forecast data that is issued to the public, observational data from automatic weather stations (AWS) across Australia, unedited data from the ECMWF's high resolution 10-day forecast model (HRES), and unedited model data from the Australian Community Climate and Earth System Simulator (ACCESS). ECMWF and ACCESS are two of the model guidance products most commonly used by Australian forecasters for winds.

ACCESS is a nested model: in this study we consider just the regional model component covering the Australian region from $65.0^\circ$ south to $16.95^\circ$ north, and $65.0^\circ$ east to $184.57^\circ$ east. This model runs at a $0.11^\circ$ ($\approx 12$ km) spatial resolution, with a time-step of $5$ minutes, \textcolor{red}{[The ACCESS-R documentation \citep{bom16} says the ``normal" time-step is $5$ minutes, and the ``short" time-step is 2.5 minutes. Assuming for now the ``normal" time-step is used operationally.]} The ECMWF model used by BoM forecasters is the high resolution 10-day forecast model (HRES), which runs at an $\approx 9$ km spatial resolution, with a 7.5 minute time-step \citet{ecmwf19b}.

Datasets are standardised using the same method the BoM uses when calculating routine verification statistics. The Official forecast dataset is at a 3 km resolution for Victoria and Tasmania, but a 6 km resolution for the rest of the country. \textcolor{red}{[Why just over Victoria and Tasmania? Nick says this may be because these states are much smaller, so GFE computational resources can handle a 3 km resolution here.]} Note that when forecasters load model guidance products into GFE they are interpolated or upscaled to a 6 km resolution Australia wide. \textcolor{red}{[Not certain about this, I may be confusing Deryn's email. Using a 3 km resolution for Official but 6 km for model guidance may create representation issues.]} Similarly, the Official forecast dataset is at UTC hourly time intervals, so just the ACCESS data at each hour UTC is considered. The HRES datasets the Bureau receives from ECMWF are at a three hourly time interval, so these are linearly interpolated to an hourly resolution when loaded into GFE. In this study, and in general throughout the BoM, verification is performed on these interpolated or upscaled datasets, i.e. on the datasets that forecasters use within GFE.
 
Australian AWS typically record wind and direction each minute. After basic quality control, 10 minute averages of speed and direction are taken at each station at each hour UTC, usually over the ten minutes leading up to each hour. When calculating verification statistics, each station is matched with the nearest 3 or 6 km grid-point in the datasets described above.         
 
Both ACCESS and ECMWF use parametrisation schemes to simulate sub-grid scale boundary layer turbulence, and the resultant mixing. ACCESS uses the schemes of \citet{lock00} and \citet{louis79} for unstable and stable boundary layers respectively \citep{bom10}. ECMWF use similar schemes that they develop in-house \citep{ecmwf19a}. This study considers the austral winter months of June, July and August 2018. This short time period was chosen to reduce the effect of changing seasonal and climatic conditions, changing forecasting practice and staff, and of developments to the ACCESS and ECMWF models.

\begin{figure*}
\centering
\includegraphics[width=39pc]{map.pdf}
\caption{Locations of the automatic weather stations used in this study. Stars indicate capital city airport stations. Height and depth shading intervals every 200 and 1000 m, respectively.}
\label{Fig:map}
\end{figure*}

\subsection{Assessing Diurnal Cycles}
Forecasters edit model guidance wind data to account for under-resolved sea-breezes and boundary layer mixing processes. Instead of attempting to assess each type of edit individually, we study the overall diurnal signal by subtracting a twenty hour centred running mean \textit{background wind} from each zonal and meridional hourly wind data point, to create wind \emph{perturbation} datasets. 

One measure of the performance of the Official, ACCESS and ECMWF diurnal cycles is to compare the Euclidean distances of the perturbations at each hour with the corresponding AWS perturbations. For example, to assess whether the Official forecast perturbations, $\boldsymbol{u}_{\text{O}}$, or ACCESS perturbations, $\boldsymbol{u}_{\text{A}}$, best match the observed AWS perturbations, $\boldsymbol{u}_{\text{AWS}}$, we calculate the \textit{Wind Perturbation Index} (WPI), defined by 
\begin{equation}
\text{WPI}_\text{OA} = \left\lvert \boldsymbol{u}_{\text{AWS}}-\boldsymbol{u}_{\text{A}} \right\rvert - \left\lvert \boldsymbol{u}_{\text{AWS}}-\boldsymbol{u}_{\text{O}} \right\rvert. \label{Eq:WPI}
\end{equation} 
The analogously defined quantities $\text{WPI}_\text{OE}$ and $\text{WPI}_\text{EA}$ can then be used to provide a comparison of the Official and ECMWF perturbations, and of the ACCESS and ECMWF perturbations, respectively. We can then take means of the WPI on an hourly basis; i.e.~all the 00:00 UTC WPI values are averaged, all the 01:00 UTC values are averaged, and so forth, and denote such an average by $\overline{\text{WPI}}$. 

$\overline{\text{WPI}}$ is the difference of two mean absolute errors. A $\overline{\text{WPI}}_\text{OA}$ value of $0.5$ kn at 00:00 UTC means that the Official 00:00 UTC perturbations are, on average, $0.5$ kn closer to the observed perturbations than are those of ACCESS. The $\text{WPI}$ compares just \textit{one aspect} of the Official forecast with model guidance. Any statements about performance made throughout this paper refer solely to WPI, or subsequently defined metrics, and no claim is being made that these are sufficient to completely characterise the accuracy, or value to the user, of how the surface diurnal wind cycle is represented in competing forecasts.

Sea-breeze and boundary layer mixing processes depend crucially on the background atmospheric conditions in which they occur. By comparing wind perturbations rather than the overall wind fields we are not claiming these background conditions are irrelevant. However, when a forecaster makes an edit of a wind forecast to better resolve these processes, they are implicitly assuming that future background conditions will be close enough to climatology, or model predictions of background conditions, to justify making the edit. Thus, it makes sense to compare forecast perturbations to observed perturbations, as long as errors are interpreted as the consequence not only of how the forecaster or model resolves the diurnal cycle, but of how errors in the background state contribute to errors in the perturbations. To minimise the significance of background state errors, this study focuses exclusively on lead-day one forecasts.

Given the large degree of turbulence and unpredictable variability in both the AWS, Official, and model datasets, care must be taken to ensure we do not pre-emptively conclude Official has outperformed the model guidance when $\overline{\text{WPI}}>0$ purely by chance. The method for estimating confidence in $\overline{\text{WPI}}$ is based on a method proposed by \citet{griffiths17}. Time series formed from the WPI values at a particular time, say 00:00 UTC, across the three month time period, are treated as an independent random sample of a random variable $W$. The sampling distribution for each $\overline{\text{WPI}}$ can be modelled by a Student's $t$-distribution, and from this we calculate the probability that $W$ is positive, denoted $\pr\left(W > 0\right)$. Although temporal autocorrelations of WPI, i.e.~correlations between WPI values at a particular hour from one day to the next, are in practice small or non-existent thanks to how WPI is defined, they are still accounted for by reducing the ``effective" sample size to $ n \left(1-\rho_1\right)/\left(1+\rho_1\right)$, where $n$ is the actual sample size and $\rho_1$ is the lag-1 autocorrelation \citep{zwiers95,wilks11}. In the standard language of statistical hypothesis testing, we would reject the null hypothesis that $W=0$ at significance level $\alpha$ if $\pr(W>0) > 1-\frac{\alpha}{2}$ or $\pr(W<0) > 1-\frac{\alpha}{2}$. However, in this we prefer to simply state the value of $\pr(W>0)$, referring to this as a \textit{confidence score}, and noting $\pr(W<0) = 1- \pr(W>0)$. We say Official outperforms model guidance with ``high confidence" if $\pr(W>0) \geq 95\%$, or that model guidance outperforms Official with ``high confidence" if $\pr(W>0) \leq 5\%$, with high confidence implicit whenever it is not explicitly mentioned.

To investigate the consequences of the representation and mesoscale verification challenges discussed in section \ref{Sec:Introduction}, we apply upscaling \citep{ebert08}, where forecast and observational data are first averaged to coarser spatiotemporal scales before being compared. The finest spatial scale we consider is that of the individual station. This study focuses on the 8 capital city airport stations, marked by stars in Fig.~\ref{Fig:map}, as their high operational significance means that they are typically the most accurate and well maintained. The next spatial scale is formed by taking the 10 stations closest to each capital city airport station, with some flexibility allowed to ensure stations are roughly parallel to the nearest coastline. These station groups are referred to as the \textit{airport station groups}. The coarsest spatial scale is formed by taking all stations within 150 km of the nearest coastline, and grouping these by state, as Australian forecasts are currently produced on a state by state basis. The Western Australian coastline is subdivided into three pieces, and stations along the Gulf of Carpentaria, north Queensland Peninsula, and Tasmanian coastlines are neglected, in order to ensure each station group corresponds to an approximately linear segment of coastline, so as to best resolve the land-sea breeze signal after spatial averaging \citep[e.g.][]{vincent16}. These eight station groups are referred to as the \textit{coastal station groups}.

We also consider both daily and seasonal time scales. For daily time scales, we either consider just the individual airport stations, or modify the definition of WPI in equation (\ref{Eq:WPI}) so that each perturbation dataset is first spatially averaged over either the airport or coastal station groups. Confidence scores are calculated for the airport and coastal station groups in the same way as for the single airport stations, treating the spatially averaged data as a single time series. This provides a conservative way to deal with spatial correlation between the stations in each group \citep{griffiths17}. 

For the seasonal scale comparison we define the \textit{Climatological Wind Perturbation Index} (CWPI) by
\begin{equation}
\text{CWPI}_{\text{OA}} = \left\lvert \overline{\boldsymbol{u}}_{\text{AWS}}-\overline{\boldsymbol{u}}_{\text{O}} \right\rvert - \left\lvert \overline{\boldsymbol{u}}_{\text{AWS}}-\overline{\boldsymbol{u}}_{\text{A}} \right\rvert,
\end{equation}
where the over-bars denote temporal averages of the perturbations at a particular hour, over June, July and August 2018. These temporally averaged perturbations represent the climatological diurnal wind cycle over the three month study period for each dataset. $\text{CWPI}_{\text{OE}}$ and $\text{CWPI}_{\text{EA}}$ are defined analogously. The three spatial scales are considered in the same way as for WPI, with the spatial average taken before the temporal average. Uncertainty in the CWPI is estimated through bootstrapping \citep{efron79}. This is done by performing resampling with replacement on the underlying perturbation datasets, and calculating the CWPI multiple times using these resampled datasets. This provides a distribution of CWPI values, which analogously to with WPI, we treat as a sample from a random variable $C$, and use this to estimate $\pr\left(C > 0\right)$.

Although the WPI and CWPI provide quantitive information on the accuracy of the diurnal cycle at different times of day, they do not provide much information on the structure of the diurnal wind cycles of each dataset. \citet{gille05} obtained summary statistics on the observed structure of the climatological diurnal wind cycles across the globe by using linear regression to calculate the coefficients $u_i$, $v_i$ $i=0,1,2$, for the fits 
\begin{align}
u &= u_0 + u_1 \cos(\omega t) + u_2 \sin(\omega t), \label{Eq:u_h} \\
v &= v_0 + v_1 \sin(\omega t) + v_2 \sin(\omega t), \label{Eq:v_h}
\end{align}
where $\omega$ is the angular frequency of the earth and $t$ is the local solar time in seconds. These fits trace out ellipses in the $x,y$ plane, and descriptive metrics, like the eccentricity of the ellipse, and the angle the semi-major axis makes with the horizontal, can be calculated directly from the coefficients $u_1$, $u_2$, $v_1$ and $v_2$. \citet{gille05} applied this fit to scatterometer data, which after temporal averaging resulted in just four zonal and meridional values per location, and as such the fit performed very well.  

However, equations (\ref{Eq:u_h}) and (\ref{Eq:v_h}) do not provide a good fit for hourly wind data, primarily because they assume a twelve hour symmetry in the evolution of the diurnal cycle. In practice, asymmetries between daytime heating and nighttime cooling \citep[e.g.][]{svensson11} result in surface wind perturbations accelerating rapidly just after sunrise, but remaining comparatively stagnant at night (e.g.~Fig.~\ref{Fig:clim_hodo}). Thus, we instead fit the equations
\begin{align}
u &= u_0 + u_1 \cos(\alpha(\psi,t)) + u_2 \sin(\alpha(\psi,t)), \label{Eq:u} \\
v &= v_0 + v_1 \sin(\alpha(\psi,t)) + v_2 \sin(\alpha(\psi,t)), \label{Eq:v}
\end{align}
to the climatological perturbations, with $\alpha$ the function from $[0,24) \times [0, 2\pi) \to [0, 2\pi)$ given by
\begin{equation}
\alpha(\psi,t) \equiv \pi \left[\sin\left( \pi \frac{(t - \psi)  \bmod 24}{24} - \frac{\pi}{2} \right) + 1 \right], \label{Eq:alpha}
\end{equation}
with $t$ the time in units of hours UTC, and $\psi$ providing the time when the wind perturbations vary least with time. For each climatological diurnal wind cycle, we solve for the seven parameters $u_0$, $u_1$, $u_2$, $v_0$, $v_1$, $v_2$ and $\psi$ using nonlinear regression, performed using the \texttt{least\_squares} function from the \texttt{scipy.optimize} python module \citep{scipy19}.

\citet{gille05} fit equations (\ref{Eq:u_h}) and (\ref{Eq:v_h}) to the temporally averaged wind fields, so that $\left(u_0, v_0\right)$ could be interpreted as the mean wind over the study's time period, and the remaining terms providing the climatological diurnal perturbations. In this study we fit equations (\ref{Eq:u}) and (\ref{Eq:v}) to the climatological perturbations themselves, with $\left(u_0, v_0\right)$ now necessary to offset the asymmetry introduced by $\alpha$, i.e.~to ensure the time integral of the fitted perturbation values is approximately zero. 

\section{Results}
\label{Sec:Results}
In this section, the methods described in section \ref{Sec:Methods} are applied to Australian forecast and station data over the months of June, July and August 2018. First, absolute errors are compared on a daily basis using the Wind Perturbation Index (WPI) at three different spatial scales. Second, overall seasonal biases during this time period are assessed using the Climatological Wind Perturbation Index (CWPI), and by comparing structural indices derived from ellipses fitted to the climatological wind perturbations.

\subsection{Daily Comparison}
\label{Sec:Daily}
Figure \ref{Fig:wpi_coastal} provides the WPI values and confidence scores for the coastal station groups for $\overline{\text{WPI}}_\text{OA}$, $\overline{\text{WPI}}_\text{OE}$ and $\overline{\text{WPI}}_\text{EA}$, which represent the the Official versus ACCESS, Official versus ECMWF, and ECMWF versus ACCESS comparisons, respectively. The results indicate that for the majority of station groups and hours, both the unedited ACCESS and ECMWF models outperform the Official forecast. The lowest $\overline{\text{WPI}}$ values occur at the NT station group at 23:00 and 00:00 UTC for both $\overline{\text{WPI}}_\text{OA}$ and $\overline{\text{WPI}}_\text{OE}$. Although Official outperforms at least one of ACCESS or ECMWF at multiple times and station groups, the only group and time where it outperforms both is 05:00 UTC over the South WA station group, although the $\overline{\text{WPI}}$ values are comparatively low. ECMWF generally outperforms ACCESS from 10:00 - 14:00 UTC, with the South WA station group being the main exception.    

\begin{figure*}
\centering
\includegraphics[width=39pc]{wpi_coastal.pdf}
\caption{Heatmaps of $\overline{\text{WPI}}$ values, a), c), e), and confidence scores, b), d), f), for each coastal station group and hour of the day: Official versus ACCESS, a) and b), Official versus ECMWF, c) and d), ECMWF versus ACCESS, e) and f). Positive $\overline{\text{WPI}}$ values indicate that the former dataset in each pair is on average $\overline{\text{WPI}}$ kn closer to observations than the latter dataset is. Confidence scores provide the probability the population or ``true" value of $\overline{\text{WPI}}$ is greater than zero.}
\label{Fig:wpi_coastal}
\end{figure*}

\begin{figure}
\centering
\includegraphics[width=19pc]{case_studies_nt.pdf}
\caption{Time series, a), of $\overline{\text{WPI}}_\text{OA}$ and $\overline{\text{WPI}}_\text{OE}$ for the NT station group at 23:00 UTC, and b), hodographs showing hourly changes in winds, and c), wind perturbations, at the NT station group on the 3\textsuperscript{rd} of July 2018.} 
\label{Fig:case_studies_nt}
\end{figure}

Figures \ref{Fig:case_studies_nt} and \ref{Fig:case_studies_wa} provide case studies of the NT and South WA station groups, respectively. Figure \ref{Fig:case_studies_nt} a) provides a time series of WPI for the NT station group at 23:00 UTC. The time series shows significant temporal variability, with WPI frequently dropping below $-2$ kn. Figures \ref{Fig:case_studies_nt} b) and c) show hodographs of the winds and wind perturbations, respectively, at each hour UTC on the 3\textsuperscript{rd} of July, which provides an interesting example. 

Figure \ref{Fig:case_studies_nt} b) shows that the Official wind forecast on this day was likely based on edited ACCESS from 00:00 to 06:00 UTC, then edited ECMWF from 07:00 to 13:00 UTC, then unedited ACCESS from 15:00 to 21:00 UTC. At 22:00 and 23:00 UTC, the Official winds acquire stronger east-northeasterly components than the other datasets. Figure \ref{Fig:perth_sounding} a) shows the first ten values from wind soundings at Darwin Airport at 12:00 UTC on July 3\textsuperscript{rd} and 00:00 UTC on July 4\textsuperscript{th}. In both instances the winds are east-southeasterly, and so the rapidly changing wind perturbations at 22:00 UTC in the Official forecast likely reflect a boundary layer mixing edit that has been applied either too early, or has strengthened the southeasterly component of the winds too much. Similar issues create low WPI scores on the 8\textsuperscript{th} of June and 9\textsuperscript{th} and 10\textsuperscript{th} of July.

Figure \ref{Fig:case_studies_wa} a) provides a time series of WPI for the South WA station group at 05:00 UTC. As with the NT station group there is significant temporal variability, with WPI frequently exceeding 1 kn. Figures \ref{Fig:case_studies_wa} b) and c) provide hodographs of the winds and wind perturbations, respectively, on the 9\textsuperscript{th} of June, which is an interesting example. The perturbation hodograph shows both ECMWF and ACCESS under-predicting the amplitude of the diurnal wind cycle on this day. Figure \ref{Fig:perth_sounding} shows wind soundings at Perth Airport, the nearest station to provide wind soundings, between 12:00 UTC on the 8\textsuperscript{th} June and 12:00 UTC on the 9\textsuperscript{th} June. The 8\textsuperscript{th} June 12:00 UTC sounding shows surface northerlies of around $6$ kn, becoming west to northwesterlies of over 20 kn $2.4$ km above the surface. However, the subsequent sounding at 00:00 UTC on the 9\textsuperscript{th} of June shows that the winds acquire a strong northerly component of 30 kn in the first 500 m of the atmosphere, with the final sounding indicating a strong northwesterly wind at 725 m persisting until 12:00 UTC. In Fig.~\ref{Fig:case_studies_wa} c), the Official perturbations from 04:00 to 07:00 UTC show stronger westerly perturbations than either ACCESS or ECMWF, improving the amplitude of Official's diurnal wind cycle. However, the AWS perturbations are more northerly than those of Official, and so the Official forecast winds have been strengthened in a slightly incorrect direction. One explanation for this discrepancy is that the Official forecast has been edited based on the June 8\textsuperscript{th} 12:00 UTC sounding, with the winds above the surface changing direction in the subsequent 12 hours. A similar explanation can be given for the high WPI scores on the  3\textsuperscript{rd} of August, although in this case the Official forecast slightly improves both the magnitude and direction of the 05:00 UTC wind perturbations.

\begin{figure}
\centering
\includegraphics[width=19pc]{case_studies_wa.pdf}
\caption{As in Fig.~\ref{Fig:case_studies_nt}, but for, a), the South WA station group at 05:00 UTC, and b) and c), the winds and wind perturbations over the South WA station group on the 9\textsuperscript{th} June 2018.} 
\label{Fig:case_studies_wa}
\end{figure}

\begin{figure*}
\centering
\includegraphics[width=33pc]{perth_sounding.pdf}
\caption{Wind soundings at, a), Darwin Airport, and b), Perth Airport.}
\label{Fig:perth_sounding}
\end{figure*}

\begin{figure*}
\centering
\includegraphics[width=39pc]{airport_wpi.pdf}
\caption{The $\overline{\text{WPI}}_\text{OE}$ values, a) and c), and confidence scores, b) and d), for the airport stations, a) and b), and airport station groups, c) and d).}
\label{Fig:airport_wpi}
\end{figure*}

Fig.~\ref{Fig:airport_wpi} presents the $\overline{\text{WPI}}$ values and confidence scores for the airport stations, and airport station groups, for just the Official versus ECMWF comparison, i.e.~$\overline{\text{WPI}}_\text{OE}$. The results for the airport stations are noisier than the results for the coastal station groups in Figs.~\ref{Fig:wpi_coastal} c) and d), although they share some similarities; for instance, Official outperforms ECMWF at 01:00 and 02:00 UTC at both the Darwin airport station and the NT station group. There are four other instances where Official outperforms ECMWF with at least $90\%$ confidence, but this has a high likelihood of occurring purely by chance due to the \textit{multiplicity problem} \citep[p. 178]{wilks11}.

For the airport station groups, ECMWF outperforms Official for the majority of locations and times. The main exception is the Darwin airport station group, where Official outperforms ECMWF at 02:00 UTC, and there is ambiguity as to whether Official or ECMWF performs better at 01:00, 03:00 and 04:00 UTC, and from 15:00 to 22:00 UTC. In the analogous $\overline{\text{WPI}}_\text{OA}$ Official versus ACCESS comparisons (not shown), the airport station results are similarly noisy, although the airport station group results show slightly fewer occasions overall where ACCESS outperforms Official, than ECMWF does. 

\begin{figure*}
\centering
\includegraphics[width=39pc]{airport_wpi_EA.pdf}
\caption{As in Fig.~\ref{Fig:airport_wpi}, but for the $\overline{\text{WPI}}_\text{EA}$ values and confidence scores.}
\label{Fig:airport_wpi_EA}
\end{figure*}

Figure \ref{Fig:airport_wpi_EA} is analogous to Fig.~\ref{Fig:airport_wpi}, but for the ECMWF versus ACCESS comparison. As with Fig.~\ref{Fig:airport_wpi}, the results for the airport stations are somewhat noisy, but more often than not show that ECMWF outperforms ACCESS. The results for the airport station group show ECMWF usually outperforms ACCESS, the main exceptions being the Darwin and Canberra airport station groups.  

\subsection{Seasonal Comparison}
\label{Sec:Seasonal}
Figure \ref{Fig:cwpi_coastal} provides the Climatological Wind Perturbation Index (CWPI) values and confidence scores for the coastal station groups for $\text{CWPI}_\text{OA}$, $\text{CWPI}_\text{OE}$ and $\text{CWPI}_\text{EA}$, which represent the the Official versus ACCESS, Official versus ECMWF, and ECMWF versus ACCESS comparisons, respectively. At the NT station group Official outperforms both ACCESS and ECMWF at 03:00 UTC with confidence $\geq 93\%$. However, both ACCESS and ECMWF outperform Official at 23:00 and 00:00 UTC, consistent with the $\overline{\text{WPI}}$ results of Fig.~\ref{Fig:wpi_coastal}. The NT station group results are discussed in more detail in section \ref{Sec:Discussion}.

At the North WA station group at 01:00, 03:00 and 04:00, Official outperforms ACCESS with confidence scores of 77, 78 and 90\%, respectively; Official also outperforms ECMWF at 01:00 and 02:00 UTC with confidence scores above 99\%. Figure \ref{Fig:clim_hodo} a) shows that ECMWF's poor performance at 01:00 and 02:00 UTC is simply due to its linear interpolation at these times, whereas Official's very slight outperformance of ACCESS at 01:00, 03:00 and 04:00 is due to ACCESS's climatological diurnal cycle being slightly out of phase with that of the AWS observations, and the Official forecast correcting for this somewhat. Both Official and ECMWF slightly exaggerate the magnitude of the climatological sea-breeze, which peaks around 09:00 UTC, with ACCESS performing well in this respect.

At the South WA station group from 01:00 to 05:00 UTC, Official outperforms ECMWF with confidence scores of at least $88\%$. Figure \ref{Fig:clim_hodo} b) shows that ECMWF underestimates the westerly perturbations at these times, with these perturbations likely associated with boundary layer mixing processes, as discussed in section \ref{Sec:Results} \ref{Sec:Daily}. Each of Official, ACCESS and ECMWF noticeably underestimate the amplitude of the diurnal cycle between 02:00 and 10:00 UTC, including both the westerly perturbations and the southerly sea-breeze perturbations. 

At the NSW station group from 17:00 to 19:00 UTC, Official outperforms both ACCESS and ECMWF with confidence scores of at least least 95\% and 75\%, respectively. Figure \ref{Fig:clim_hodo} c) shows that these times correspond to ``dimples" in the perturbation hodographs that are present in all four datasets. The Official hodograph closely resembles that of ACCESS, except for this dimple, which has been exaggerated relative to ACCESS. \textcolor{red}{Don't know what is going on here.} Figure \ref{Fig:clim_hodo} c) also shows that although ECMWF exaggerates the amplitude of the easterly sea-breeze perturbations, it captures the narrower shape of the AWS hodograph better than Official or ACCESS.

At the SA station group from 02:00 to 05:00 UTC and 09:00 to 12:00 UTC, Official outperforms both ACCESS and ECMWF, although confidence scores do not exceed 88\% and 65\% respectively. Figure \ref{Fig:clim_hodo} d) shows that although the Official forecast captures the amplitude of the perturbations from 01:00 to 05:00 UTC almost perfectly, its diurnal cycle is out of phase with that of the AWS during this period, explaining why Official only slightly outperforms ACCESS in the results of Figures \ref{Fig:cwpi_coastal} a) and b).

\begin{figure*}
\centering
\includegraphics[width=39pc]{cwpi_coastal.pdf}
\caption{As in Fig.~\ref{Fig:wpi_coastal}, but for the CWPI values and confidence scores.}
\label{Fig:cwpi_coastal}
\end{figure*}

\begin{figure*}
\centering
\includegraphics[width=33pc]{clim_hodo.pdf}
\caption{Average wind perturbations over June, July and August 2018 for the, a), North WA, b) South WA, c) NSW and d), SA coastal station groups.}
\label{Fig:clim_hodo}
\end{figure*}

\begin{figure*}
\centering
\includegraphics[width=39pc]{airport_cwpi.pdf}
\caption{As in Fig.~\ref{Fig:airport_wpi}, but for the CWPI values and confidence scores.}
\label{Fig:airport_cwpi}
\end{figure*}

For constrast, Fig.~\ref{Fig:airport_cwpi} presents the CWPI values and confidence scores for $\text{CWPI}_\text{OE}$, which represents the Official versus ECMWF comparison, for the airport stations, and airport station groups. These results show much greater similarity with the Official versus ECMWF comparisons at the coastal station groups shown in Figs.~\ref{Fig:cwpi_coastal} c) and d), than do the analogous $\overline{\text{WPI}}$ results in Fig.~\ref{Fig:airport_wpi} and Figs.~\ref{Fig:wpi_coastal} c) and d). This likely because the temporal averaging has reduced the additional unpredictable variability in Official, revealing biases in Official and ECMWF that are partly shared across the three spatial scales. This point is discussed further in section \ref{Sec:Discussion}. The analogous CWPI comparisons with ACCESS (not shown) are more ambiguous, although are generally more favourable for Official than those for $\overline{\text{WPI}}$. For example Official outperforms both ACCESS and ECMWF at Darwin Airport from 02:00 to 03:00 and 15:00 to 17:00 UTC with at least $90\%$ confidence. However, Official performs less well compared to ACCESS over the airport station groups, with CWPI values close to zero for most times and station groups, but ACCESS now strongly outperforming Official over the Darwin Airport station group.  

\begin{figure}
\centering
\includegraphics[width=19pc]{r_squared.pdf}
\caption{$R^2$ values as percentages for the fit of equation (\ref{Eq:u}) to the zonal perturbations, a), c) and e), and equation (\ref{Eq:v}) to the meridional perturbations, b), d) and f), for the airport stations, a) and b), airport station groups, c) and d), and coastal station groups, e) and f).}
\label{Fig:r_squared}
\end{figure}

Note that the hodographs in Fig.~\ref{Fig:clim_hodo} are roughly elliptical in shape, suggesting that descriptive quantities can be estimated by fitting equations (\ref{Eq:u}) and (\ref{Eq:v}) to the zonal and meridional climatological perturbations, as described in section \ref{Sec:Methods}. Figure \ref{Fig:r_squared} provides the $R^2$ values for the fits of the zonal and meridional perturbations to equations (\ref{Eq:u}) and (\ref{Eq:v}), respectively. The fit performs best at the coastal station group spatial scale, with $R^2$ generally above $95\%$. It also performs well at the airport station and airport station group scales, with a few exceptions, including the ACCESS and Official meridional perturbations at the Canberra airport station group, and the ECMWF zonal perturbations at Melbourne airport. 

\begin{figure*}
\centering
\includegraphics[width=39pc]{ellipse_fits.pdf}
\caption{Metrics derived from fitting the elliptical equations (\ref{Eq:u}) and (\ref{Eq:v}) to the climatological perturbations: maximum perturbation speed, a), e) and i), eccentricity, b), f) and j), orientation, c), g) and k), and time of maximum perturbation, d), h) and l), for the airport stations, a) to d), airport station groups, e) to h), and coastal station groups, i) to l).}
\label{Fig:ellipse_fits}
\end{figure*}

The ellipse fits are used to derive four descriptive quantities: the maximum perturbation speed, the eccentricity of the fitted ellipse, the angle the fitted ellipse's semi-major axis makes with lines of latitude, and the time maximum perturbation speed occurs. Figure \ref{Fig:ellipse_fits} provides these four quantities for each dataset and location across the three spatial scales. A variety of structural differences are apparent at a number of locations and scales. For example, Fig.~\ref{Fig:ellipse_fits} a) shows that at Brisbane airport, the maximum AWS perturbation is at least $1$ kn greater than Official, ACCESS and ECMWF, and Fig.~\ref{Fig:ellipse_fits} c) shows that the orientation of the AWS fitted ellipse is at least 20 degrees anti-clockwise from the other datasets. Figures \ref{Fig:ellipse_hodo} a) and b) show hodographs of the Brisbane airport perturbation climatology and ellipse fit, respectively. Although the ellipse fits suppress some of the asymmetric details, they capture the amplitudes and orientations of the real climatological diurnal cycles well. In this case the results show that the average AWS sea-breeze approaches from the northeast, whereas the Official, ECMWF and ACCESS sea-breezes approach more from the east-northeast. To check whether this just represents a direction bias of the Brisbane Airport station, Fig.~\ref{Fig:ellipse_fits} shows the climatological perturbations at the nearby Spitfire Channel station (see Fig.~\ref{Fig:map} for the location of this station, and other stations referred to in this section). While the amplitude bias is smaller at Spitfire Channel than Brisbane Airport, the directional bias is at least as high. A similar directional bias is evident at the nearby Inner Beacon station (not shown), although the bias is smaller than at Spitfire Channel and Brisbane Airport. Thus, the directional bias in Official, ACCESS and ECMWF at these stations is likely genuine, and not just a consequence of biased AWS observations. Figure \ref{Fig:map} shows there are two small islands to the east of Brisbane airport; the more northwesterly orientation of the Brisbane Airport sea-breeze suggests these islands may be redirecting winds between the east coast of Brisbane and the west coasts of these islands, and that this local effect is not being captured in Official, ACCESS or ECMWF.            

\begin{figure*}
\centering
\includegraphics[width=39pc]{ellipse_hodo.pdf}
\caption{Hodographs of the climatological perturbations at Brisbane and Hobart airports, a) and d), and the associated ellipse fits, b) and e). For comparison, c) and f) provide hodographs of the climatological perturbations at Spitfire Channel and Hobart (city), respectively.}
\label{Fig:ellipse_hodo}
\end{figure*}

Another example is the Hobart Airport station. Figure \ref{Fig:ellipse_fits} c) shows that the ellipse fits for the AWS perturbations are oriented 31, 35 and 62 degrees anti-clockwise from the ECMWF, Official and ACCESS ellipse fits, respectively. Figures \ref{Fig:r_squared} a) and b) show that the ellipse fit for the AWS perturbations at Hobart airport only achieve $R^2$ values of 59\% and 68\% for the $u$ and $v$ components, respectively. However, figures \ref{Fig:ellipse_hodo} d) and e) show that the fit still captures orientations accurately, although it underestimates the maximum AWS perturbation. Figure \ref{Fig:ellipse_hodo} f) shows the climatological perturbations at the Hobart (city) station, which also show a large difference in orientation between ACCESS and AWS. Given the timing of the westerly perturbations in ACCESS, and the fact that the prevailing winds around Tasmania are westerly, these results suggest that ACCESS is exaggerating the boundary layer mixing processes involved in the diurnal cycle around Hobart.

The South WA station group also provides an interesting example. Here the ACCESS and Official ellipse fits are oriented at least 49 degrees anti-clockwise from those of AWS and ECMWF, and the ECMWF perturbations peak between 1.2 and 2.5 hours after the other datasets. These differences occur because eccentricity values are low for this station group, and Figure \ref{Fig:clim_hodo} b) shows that the westerly perturbations associated with boundary layer mixing are weaker for ECMWF that the other datasets. A similar issue affects the VIC station group, explaining why the AWS ellipse fit is oriented at least 49 degrees anti-clockwise from those of the other datasets.  

The Darwin Airport, Darwin Airport station group, and NT station group provide further examples. In these cases there are timing differences between the perturbation maximums of up to 8.2 hours. Figure \ref{Fig:nt_ellipse_hodo} shows that these differences occur because for some datasets, the later north to northwesterly sea-breeze perturbations dominate the diurnal wind cycle, but for other datasets the earlier easterly to southeasterly boundary layer mixing effects dominate. 

\begin{figure*}
\centering
\includegraphics[width=39pc]{nt_ellipse_hodo.pdf}
\caption{Hodographs of the climatological perturbations at, a), Darwin Airport, b) the Darwin Airport station group, and c), the NT coastal station group.}
\label{Fig:nt_ellipse_hodo}
\end{figure*}

\section{Discussion}
\label{Sec:Discussion}
The results of section \ref{Sec:Results} may have implications for forecasting practice. If the goal of land-sea breeze and boundary layer mixing edits is to reduce absolute errors in the following day's forecast of the surface wind fields, then a necessary (but not sufficient) condition for this to occur is for these edits to at least reduce the absolute errors in the diurnal component of the surface wind fields. However, the WPI comparisons in Figs.~\ref{Fig:wpi_coastal} and \ref{Fig:airport_wpi} suggest that this is only possible when absolute error is calculated at coarse spatial scales. If the Official forecast is based, at least partly, on an edited high resolution model guidance dataset like ACCESS, then due to the mesoscale verification issues discussed in section \ref{Sec:Introduction}, the larger absolute errors associated with a higher resolution model completely mask the effect of the edits, with a lower resolution unedited model like ECMWF scoring better overall. While the CWPI results in Figs.~\ref{Fig:cwpi_coastal} and \ref{Fig:airport_wpi} suggest that forecaster edits can improve the accuracy of diurnal wind cycles in a climatological sense, it is not clear if these improvements have operational significance. 

To investigate these ideas further, consider first just the zonal components of the AWS and Official wind perturbations, denoted by $u_\text{AWS}$ and $u_\text{O}$ respectively. Considering just the values at a particular hour UTC, at a particular station, over the entire June, July, August time period, the mean square error  $\mse\left(u_\text{AWS}, u_\text{O}\right) = \overline{\left(u_\text{AWS} - u_\text{O}\right)^2}$ can be decomposed
\begin{equation}
\mse\left(u_\text{AWS}, u_\text{O}\right) = \underbrace{\var\left(u_\text{AWS}\right) + \var\left(u_\text{O}\right) - 2 \cdot \covar\left(u_\text{AWS}, u_\text{O}\right)}_{\var\left(u_\text{AWS} - u_\text{O}\right)} + \underbrace{\left(\overline{u}_\text{AWS} - \overline{u}_\text{O}\right)^2}_{\text{squared bias}} \label{Eq:MSE}
\end{equation}
where $\var$, $\covar$ and over-bars denote the sample variance, covariance and mean respectively. The first three terms are the total variance of $u_\text{AWS} - u_\text{O}$, whereas the last term is the square of the bias between $u_\text{AWS}$ and $u_\text{O}$. Note that the mean square error $\mse\left(u_\text{AWS}, u_\text{O}\right)$ is closely related to $\overline{\text{WPI}}$, which is the difference between the mean absolute error of Official and AWS, and a model guidance dataset and AWS. Similarly, the CWPI is closely related to the squared bias component $\left(\overline{u}_\text{AWS} - \overline{u}_\text{O}\right)^2$ of the mean square error. Equation (\ref{Eq:MSE}) can also be applied to wind perturbations that have first been spatially averaged over a station group, and to $\mse\left(u_\text{AWS}, u_\text{E}\right)$ and $\mse\left(u_\text{AWS}, u_\text{A}\right)$, where $u_\text{E}$ and $u_\text{A}$ are the ECMWF and ACCESS zonal perturbations, respectively. 

\begin{figure*}
\centering
\includegraphics[width=39pc]{error_decomp.pdf}
\caption{Mean square error between the ECMWF and AWS zonal perturbations $\overline{\left(u_\text{AWS} - u_\text{E}\right)^2}$ decomposed into the total variance $\var\left(u_\text{AWS} - u_\text{E}\right)$ and squared bias $\left(\overline{u}_\text{AWS} - \overline{u}_\text{E}\right)^2$ terms of equation (\ref{Eq:MSE}), a), e) and i), and analogously for the mean square error between the Official and AWS zonal perturbations $\overline{\left(u_\text{AWS} - u_\text{O}\right)^2}$, b), f) and j). Also, the total variance term $\var\left(u_\text{AWS} - u_\text{E}\right)$ decomposed into the $\var\left(u_\text{AWS}\right)$, $\var\left(u_\text{E}\right)$ and  $- 2 \cdot \covar\left(u_\text{AWS}, u_\text{E}\right)$ terms, c), g) and k), and analogously for $\var\left(u_\text{AWS} - u_\text{O}\right)$, d), h) and l). Decompositions given for Darwin Airport, a) to d), the Darwin Airport station group, e) to h), and the NT coastal station group, i) to l).}
\label{Fig:error_decomp}
\end{figure*}

Figure \ref{Fig:error_decomp} shows each term in the mean square error decomposition of equation \ref{Eq:MSE} for both $\mse\left(u_\text{AWS}, u_\text{O}\right)$ and $\mse\left(u_\text{AWS}, u_\text{E}\right)$, for Darwin Airport, the Darwin station group, and the NT station group. This region provides an interesting case study because Fig.~\ref{Fig:airport_wpi} shows that Official has some skill at both Darwin Airport and over Darwin Airport station groups, in contrast to most other locations. At Darwin Airport, $\mse\left(u_\text{AWS}, u_\text{O}\right)$ exceeds $\mse\left(u_\text{AWS}, u_\text{E}\right)$ from 04:00 to 16:00 UTC due to higher total variance, whereas outside of these times $\mse\left(u_\text{AWS}, u_\text{E}\right)$ exceeds $\mse\left(u_\text{AWS}, u_\text{O}\right)$ due to larger bias. The higher total variance of $u_\text{AWS} - u_\text{O}$ occurs because $\var\left(u_\text{O}\right) > \var\left(u_\text{E}\right)$. This additional variability is mostly random from 04:00 to 14:00 UTC, i.e. $u_\text{O}$ is not sufficiently correlated with $u_\text{AWS}$ at these times for the additional variability of $u_\text{O}$ to produce a reduction in mean square error. Thus, while the bias between Official and AWS is lower, or about the same, as that between ECMWF and AWS, the higher random variability of Official results in higher mean square error for most of the day. Figure \ref{Fig:error_decomp_v} shows similar conclusions can be drawn for the meridional perturbations at Darwin Airport, although in this case $\var\left(u_\text{O}\right) > \var\left(u_\text{E}\right)$ for the entire day. Most of the difference between the WPI and CWPI scores for the Official versus ECMWF comparison at Darwin Airport in Figures \ref{Fig:airport_wpi} and \ref{Fig:airport_cwpi}, respectively, can be explained through the different mean square error and bias terms for the zonal perturbations alone. 

Figure \ref{Fig:nt_ellipse_hodo} a) shows that ECMWF's climatological perturbations at Darwin Airport underestimate the easterly perturbations from 00:00 to 03:00 UTC, which are presumably associated with boundary layer mixing processes. Official does a better job of resolving these easterly perturbations, but is generally outperformed by ECMWF in resolving the northerly sea-breeze perturbations. Similar points can be made for the Darwin and NT coastal station groups. While spatial averaging reduces a portion of the unpredictable variability in Official, Official also often has larger meridional biases at these scales compared to ECMWF. Figures \ref{Fig:nt_ellipse_hodo} and \ref{Fig:ellipse_fits} show that these biases can be explained in terms of amplitude and orientation differences between Official, ECMWF and AWS. Figures analogous to Figs. \ref{Fig:error_decomp} and \ref{Fig:error_decomp_v}, but for other locations around Australia, show similar results, but generally without large biases in the Official forecast at the coarser scales like those present in the meridional perturbations over the Darwin Airport station group and NT coastal station group.   

These examples illustrate the idea that the additional unpredictable variability introduced by a higher resolution edited forecast needs to be ``paid for" by a reduction in bias, otherwise the net result will just be an increase in error.  One obvious way to reduce the influence of unpredictable variability at higher resolutions is to move to an ensemble forecasting approach. However, computational resources typically require a choice between either a single model of higher resolution, or an ensemble of models at lower resolution, and there is a long and spirited debate in the literature about the relative merits of each \citep{brooks93}. If ensemble forecasting is not possible, careful thought must be given to precisely what scales of motion the Official forecast is intended to represent. If the end user doesn't care about the ``realism" of the forecast and simply wants the lowest errors, and if daily errors are larger with a higher resolution, edited forecast, than with a coarse model guidance product, there may be an argument for smoothing or filtering the higher resolution forecast before it is provided to the end user, assuming of course the higher resolution forecast actually reduces biases. 

Furthermore, it is unclear if the Official forecast's wind fields are intended to be regarded as predictions of the actual winds at a specific location, or a predicted Reynold's average, and if so, at what scale. If we assume the BoM's Official wind forecast intends to represent variability at hourly timescales, and horizontal scales less than 50 kms, then sea-breeze and boundary layer mixing edits appear to have little effect, because the intended reduction in error is washed out by the unpredictable turbulent variability at these scales, and lower errors can be achieved simply by using a coarser resolution unedited model forecast. However, some users may be more interested in whether the variability of the forecast wind field matches those of observations, than in whether the forecast minimises absolute error, and a higher resolution edited forecast will likely perform better than a coarse resolution model in this regard. 

Ambiguity in what the Official forecast represents, and the resulting verification challenge, points to an important role for human forecasters: post-processing and editing model data so that the forecast consistently represents what is of interest to individual users. One barrier to this is that the Official forecast is provided to the national public as a whole, which includes diverse users with different representation needs. Part of the solution may therefore be the development of a secondary economy, either within national weather services like the BoM, or in the private sector, where human forecasters ensure their forecast products consistently represent the ``filtered version of reality" of interest to the end user. This would then help address the representation problem as it applies to the verification of operational forecasts, as the individual user's representation needs could then be taken as the intended representation of the forecast, and appropriate verification metrics chosen accordingly.    

\begin{figure*}
\centering
\includegraphics[width=39pc]{error_decomp_v.pdf}
\caption{As in Fig.~\ref{Fig:error_decomp}, but for the meridional perturbations.}
\label{Fig:error_decomp_v}
\end{figure*}

\section{Conclusion}
\label{Sec:Conclusion}
In this paper we have presented a method for verifying the diurnal component of wind forecasts issued to the public, with the intended application being the assessment of the edits Australian forecasters make to model guidance datasets in order to better resolve land-sea breeze and boundary layer mixing processes. We considered two temporal scales, and three spatial scales, but the method is immediately generalisable to other scales. 

When the method is applied to Australian forecast data, the results indicate that when the Official, edited forecast, is assessed on a daily basis, it only produces lower absolute errors in the diurnal wind cycle at very coarse spatial scales of at least 500 km. Even at these scales, the improvements are isolated to particular times of day, and only apply at some locations. Furthermore, while the Official forecast can outperform the two most commonly used model guidance products ACCESS and ECMWF in the sense of absolute error, it rarely outperforms both simultaneously, suggesting that forecaster skill lies more in making the choice of model guidance than in making edits. When the Official forecast is assessed on a seasonal basis, i.e. the average or climatological diurnal wind cycle is assessed, the Official forecast performs better than when assessed on a daily basis, particularly at the single station spatial scale. However, its performance is not overwhelming, as it struggles to unambiguously produce lower absolute error than ACCESS. 

An alternative to calculating absolute errors is to assess the realism of structural features of the atmosphere, and following \citet{gille05}, we do this in an objective way by fitting ellipses to hodographs of the climatological diurnal wind cycles, and deriving structural metrics from the ellipses. In the Australian context this approach reveals structural biases in the Official forecast, including directional biases in the approach of the sea-breeze at Brisbane airport, eccentricity biases along the coast of NSW, and amplitude biases along the southwest coast of WA.  

Future research could extend this study in multiple directions. An important goal would be to identify precisely the spatial scale at which the Official forecast can produce lower absolute error on a daily basis than a coarse model like ECMWF: for Australia over the time period considered, our study shows that this occurs somewhere between our airport station group scale (50 - 200 km), and coastal station group scale (1000 - 2000 km). Another interesting question is whether the diurnal component of the Official forecast can outperform a climatological diurnal cycle calculated from previous observations. 

In summary, we have shown that forecaster edits can reduce errors in the diurnal cycle of surface winds, but only at very large spatial scales, or in a climatological sense. This scale sensitivity suggests careful thought needs to be given to how the representation problem applies to the verification of operational forecasts. A consistent answer may prove challenging for national forecasting centres to reach, due to the diverse representation needs of forecast users, and in the Australian context, due to the hybrid nature of the Official forecast.

\acknowledgments
Funding for this study was provided for Ewan Short by the Australian Research Council's Centre of Excellence for Climate Extremes (CE170100023). Datasets and software were generously provided by the Australian Bureau of Meteorology's Evidence Tasked Automation team. \textcolor{red}{(Link to Jive homepage or GitHub page?} Thanks are due to Michael Foley and Deryn Griffiths for providing support at the Bureau of Meteorology's Melbourne office, and to Craig Bishop for some helpful conversations. The code written for this study is freely available online \citep{shortGitVeri19}.

\bibliographystyle{ametsoc2014}
\bibliography{./references.bib}

\end{document}
